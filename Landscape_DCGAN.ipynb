{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0berHRUZqCa1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dense, Conv2D, Conv2DTranspose, Dropout, Flatten, ReLU, LeakyReLU, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkqMFqaB0Mtt"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8UIJ5ZIqGLo"
      },
      "outputs": [],
      "source": [
        "#!mkdir '/content/landscape-dataset/'\n",
        "#!unzip  '/content/gdrive/MyDrive/landscape dataset.zip' -d '/content/landscape-dataset/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACaOk9qp06Je"
      },
      "outputs": [],
      "source": [
        "PATH_TO_DATA = '/content/landscape-dataset/landscape dataset/'\n",
        "IMAGE_SHAPE = (128,128,3)\n",
        "FILENAMES = os.listdir(PATH_TO_DATA)\n",
        "DATASET_SIZE = len(FILENAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPXiZ1ssqGOz"
      },
      "outputs": [],
      "source": [
        "def normalize(img):\n",
        "    return (img / 127.5) - 1\n",
        "\n",
        "def latent_points(size, latent_dim):  \n",
        "    z_vector = np.random.normal(0,1, (size, latent_dim))\n",
        "    return z_vector\n",
        "\n",
        "def plot_generated_images(generator, n, latent_dim):\n",
        "    latent = latent_points(n, latent_dim)\n",
        "    imgs = (generator(latent) + 1) / 2\n",
        "\n",
        "    _, axs = plt.subplots(1, n, figsize=(12, 12))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for img, ax in zip(imgs, axs):\n",
        "        ax.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "def load_data():\n",
        "    images = np.empty((DATASET_SIZE, *IMAGE_SHAPE), dtype = np.float32)\n",
        "\n",
        "    for i, filename in tqdm(enumerate(FILENAMES)):\n",
        "        path = os.path.join(PATH_TO_DATA, filename)\n",
        "        \n",
        "        img = PIL.Image.open(path).convert('RGB').resize(IMAGE_SHAPE[:2], PIL.Image.ANTIALIAS)\n",
        "        img = normalize(np.asarray(img))\n",
        "        \n",
        "        images[i, ...] = img\n",
        "  \n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-exb5SXuqGRX",
        "outputId": "232a67e1-5b1f-4bbe-c28d-9d9b34db9267"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4319it [01:33, 46.06it/s]\n"
          ]
        }
      ],
      "source": [
        "train_images = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upYNl79OqGUF"
      },
      "outputs": [],
      "source": [
        "class DCGAN:\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.epochs = 400\n",
        "        self.latent_dim = 150\n",
        "        self.weight_init = RandomNormal(stddev=0.02)\n",
        "        self.alpha = 0.2\n",
        "        self.generator_lr = 2e-4\n",
        "        self.discriminator_lr = 2e-4       \n",
        "        self.gen_optimizer = Adam(self.generator_lr, beta_1 = 0.5)\n",
        "        self.disc_optimizer = Adam(self.discriminator_lr, beta_1 = 0.5)\n",
        "        self.generator = self.get_generator()\n",
        "        self.discriminator = self.get_discriminator()\n",
        "        self.gan = self.adversarial_model(self.generator, self.discriminator)\n",
        "\n",
        "    def generate_fake_images(self, size):\n",
        "        latent_vectors = latent_points(size, self.latent_dim)\n",
        "        images = self.generator(latent_vectors)\n",
        "        return images\n",
        "\n",
        "    def ConvBlock(self, input, filters, kernel_size, drop_rate = 0.3):\n",
        "        x = Conv2D(filters = filters, kernel_size = kernel_size, padding = 'same', strides = 2, kernel_initializer=self.weight_init)(input)\n",
        "        x = LeakyReLU(alpha = self.alpha)(x)\n",
        "        x = Dropout(drop_rate)(x)\n",
        "        return x\n",
        "\n",
        "    def UpSamplingBlock(self, input, filters):\n",
        "        x = UpSampling2D((2,2))(input)\n",
        "        x = Conv2D(filters = filters, kernel_size = 4, strides = 1, padding = 'same', kernel_initializer = self.weight_init)(x)\n",
        "        x = BatchNormalization(momentum = 0.8)(x, training = True)\n",
        "        x = LeakyReLU(alpha = self.alpha)(x)\n",
        "        return x\n",
        "\n",
        "    def get_discriminator(self):\n",
        "        input_layer = Input(IMAGE_SHAPE)\n",
        "\n",
        "        x = self.ConvBlock(input_layer, filters = 64, kernel_size = 3)\n",
        "        x = self.ConvBlock(x, filters = 128, kernel_size = 3)\n",
        "        x = self.ConvBlock(x, filters = 256, kernel_size = 3)\n",
        "        x = self.ConvBlock(x, filters = 512, kernel_size = 3)\n",
        "\n",
        "        x = Flatten()(x)\n",
        "        output = Dense(units = 1, activation='sigmoid')(x)\n",
        "\n",
        "        discriminator = Model(inputs = input_layer, outputs = output)\n",
        "        discriminator.compile(loss = 'binary_crossentropy', optimizer = self.disc_optimizer)\n",
        "        return discriminator\n",
        "\n",
        "    def get_generator(self):\n",
        "        n_units = 8*8\n",
        "\n",
        "        input_layer = Input((self.latent_dim, ))\n",
        "        x = Dense(units = n_units * 128)(input_layer)\n",
        "        x = BatchNormalization()(x, training = True)\n",
        "        x = LeakyReLU(alpha = self.alpha)(x)\n",
        "        x = Reshape((8,8,128))(x)\n",
        "\n",
        "        x = self.UpSamplingBlock(x, filters = 64)\n",
        "        x = self.UpSamplingBlock(x, filters = 128)\n",
        "        x = self.UpSamplingBlock(x, filters = 128)\n",
        "        x = self.UpSamplingBlock(x, filters = 256)\n",
        "\n",
        "        output = Conv2D(filters = 3, kernel_size = 3, padding = 'same', activation = 'tanh')(x)\n",
        "\n",
        "        generator = Model(inputs = input_layer, outputs = output)\n",
        "        return generator\n",
        "\n",
        "    def adversarial_model(self, generator, discriminator):\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        gan = Sequential([\n",
        "                generator,\n",
        "                discriminator])\n",
        "\n",
        "        gan.compile(loss = 'binary_crossentropy', optimizer = self.gen_optimizer)\n",
        "        return gan\n",
        "\n",
        "    def train(self):\n",
        "        size = self.batch_size // 2\n",
        "        k_steps = DATASET_SIZE // self.batch_size\n",
        "        std = 0.1  # initial standard deviation value for noise decay\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "          disc_loss_real = disc_loss_fake = generator_loss = 0\n",
        "          noise_decay_rate = epoch/self.epochs\n",
        "          \n",
        "          new_std = std - noise_decay_rate\n",
        "          new_std = new_std if new_std > 0 else 0\n",
        "\n",
        "          for k in tqdm(range(k_steps)):\n",
        "            real_imgs = train_images[k*size: (k+1)*size] + tf.random.normal(shape = IMAGE_SHAPE, mean=0, stddev=new_std, dtype=tf.float32)   # add gaussian noise only to the real images \n",
        "            real_imgs = tf.image.random_flip_left_right(real_imgs)\n",
        "            fake_imgs = self.generate_fake_images(size)\n",
        "\n",
        "            disc_loss_real = self.discriminator.train_on_batch(real_imgs, np.ones((size, 1)) * 0.9)    # label smoothing\n",
        "            disc_loss_fake = self.discriminator.train_on_batch(fake_imgs, np.zeros((size, 1)) + 0.1)\n",
        "\n",
        "            latent_space = latent_points(self.batch_size, self.latent_dim)\n",
        "            generator_loss = self.gan.train_on_batch(latent_space, np.ones((self.batch_size, 1)) * 0.9)\n",
        "\n",
        "          print(f'''Epoch: {epoch+1}/{self.epochs}           \n",
        "          Discriminator loss on real images: {disc_loss_real}\n",
        "          Discriminator loss on fake images: {disc_loss_fake}\n",
        "          Generator loss: {generator_loss}''')\n",
        "\n",
        "          # visualizing the results\n",
        "          for _ in range(2):\n",
        "            plot_generated_images(generator, n = 5, latent_dim = self.latent_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZdyDqBx2E3Q"
      },
      "outputs": [],
      "source": [
        "dcgan = DCGAN()\n",
        "generator = dcgan.generator\n",
        "#discriminator = dcgan.discriminator\n",
        "#gan = dcgan.gan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m77Y263pqSRg",
        "outputId": "bb02116e-2f3a-42f1-a299-9bc03215bdf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8192)              1236992   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 8192)              32768     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 16, 16, 64)        131136    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 128)       131200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 64, 64, 128)       262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2 (None, 128, 128, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 256)     524544    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 128, 128, 256)     1024      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 128, 3)       6915      \n",
            "=================================================================\n",
            "Total params: 2,328,131\n",
            "Trainable params: 2,310,595\n",
            "Non-trainable params: 17,536\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aUH1K98nqGWX",
        "outputId": "9ffa10b5-4b7b-4c76-875e-b777739b269b"
      },
      "outputs": [],
      "source": [
        "dcgan.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ySy8NIC3qGZD"
      },
      "outputs": [],
      "source": [
        "for _ in range(20):\n",
        "  plot_generated_images(generator, n = 3, latent_dim = 150)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Landscape DCGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}